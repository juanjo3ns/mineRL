diff --git a/configs/config_0.2.yml b/configs/config_0.2.yml
index e205849..b8ab122 100644
--- a/configs/config_0.2.yml
+++ b/configs/config_0.2.yml
@@ -1,4 +1,4 @@
-experiment: 'curl_0.1.3'
+experiment: 'curl_0.1.1'
 environment: 'MineRLNavigate-v0'
 batch_size: 32
 epochs: 200
@@ -9,4 +9,4 @@ curl:
   img_size: 64
   encoder_tau: 0.005
   load: yes
-  epoch: '30000.pt'
+  epoch: '65000.pt'
diff --git a/interactive.py b/interactive.py
index 6cdb899..3885c43 100644
--- a/interactive.py
+++ b/interactive.py
@@ -1,25 +1,22 @@
 import gym
 import minerl
 from time import sleep, time
+from IPython import embed
 
-env = gym.make('MineRLTreechop-v0')
+env = gym.make('MineRLNavigate-v0')
 
 # set the environment to allow interactive connections on port 6666
 # and slow the tick speed to 6666.
-# env.make_interactive(port=6666, realtime=True)
+env.make_interactive(port=6666, realtime=True)
 
 # reset the env
+env.seed(2)
 env.reset()
 ini = time()
 
 while True:
+    embed()
     action = env.action_space.sample()
     obs, reward, done, _ = env.step(action)
-    print(obs.keys())
-    for k,v in obs.items():
-        print(k, ' shape: ', v.shape)
-    sleep(2)
-    if time()-ini > 10:
-        break
 
 env.close()
diff --git a/src/CURL.py b/src/CURL.py
deleted file mode 100644
index f44cf00..0000000
--- a/src/CURL.py
+++ /dev/null
@@ -1,47 +0,0 @@
-import torch
-import torch.nn as nn
-
-class CURL(nn.Module):
-    """
-    CURL
-    """
-
-    def __init__(self, obs_shape, z_dim, batch_size, encoder, encoder_target, output_type="continuous"):
-        super(CURL, self).__init__()
-        self.batch_size = batch_size
-
-        self.encoder = encoder
-
-        self.encoder_target = encoder_target
-
-        self.W = nn.Parameter(torch.rand(z_dim, z_dim))
-        self.output_type = output_type
-
-    def encode(self, x, detach=False, ema=False):
-        """
-        Encoder: z_t = e(x_t)
-        :param x: x_t, x y coordinates
-        :return: z_t, value in r2
-        """
-        if ema:
-            with torch.no_grad():
-                z_out = self.encoder_target(x)
-        else:
-            z_out = self.encoder(x)
-
-        if detach:
-            z_out = z_out.detach()
-        return z_out
-
-    def compute_logits(self, z_a, z_pos):
-        """
-        Uses logits trick for CURL:
-        - compute (B,B) matrix z_a (W z_pos.T)
-        - positives are all diagonal elements
-        - negatives are all other elements
-        - to compute loss use multiclass cross entropy with identity matrix for labels
-        """
-        Wz = torch.matmul(self.W, z_pos.T)  # (z_dim,B)
-        logits = torch.matmul(z_a, Wz)  # (B,B)
-        logits = logits - torch.max(logits, 1)[0][:, None]
-        return logits
diff --git a/src/curl_inference.py b/src/curl_inference.py
deleted file mode 100644
index 3f4f6b8..0000000
--- a/src/curl_inference.py
+++ /dev/null
@@ -1,116 +0,0 @@
-import os
-import sys
-import cv2
-import gym
-import json
-import time
-import minerl
-import numpy as np
-import matplotlib.pyplot as plt
-
-from os.path import join
-from pathlib import Path
-from pprint import pprint
-from config import setSeed, getConfig
-
-import torch
-import torch.nn as nn
-import torch.optim as optim
-import torch.nn.functional as F
-
-from torch.utils.data import DataLoader
-from torch.utils.tensorboard import SummaryWriter
-
-from encoder import PixelEncoder
-from CURL import CURL
-
-from IPython import embed
-
-setSeed(0)
-assert len(sys.argv) == 2, "Indicate a configuration file like 'config_0.0'"
-conf = getConfig(sys.argv[1])
-
-MINERL_GYM_ENV = os.getenv('MINERL_GYM_ENV', 'MineRLNavigate-v0')
-# MINERL_GYM_ENV = os.getenv('MINERL_GYM_ENV', 'MineRLTreechop-v0')
-MINERL_DATA_ROOT = os.getenv('MINERL_DATA_ROOT', '/home/usuaris/imatge/juan.jose.nieto/mineRL/data/')
-data = minerl.data.make(MINERL_GYM_ENV, data_dir=MINERL_DATA_ROOT, num_workers=1)
-
-feature_dim = conf['curl']['embedding_dim']
-img_size = conf['curl']['img_size']
-obs_shape = (3, img_size, img_size)
-batch_size = conf['batch_size']
-
-if os.getenv('USER') == 'juanjo':
-    path_weights = Path('../weights/')
-elif os.getenv('USER') == 'juan.jose.nieto':
-    path_weights = Path('/mnt/gpid07/users/juan.jose.nieto/weights/')
-else:
-    raise Exception("Sorry user not identified!")
-
-
-pixel_encoder = PixelEncoder(obs_shape, feature_dim)
-pixel_encoder_target = PixelEncoder(obs_shape, feature_dim)
-
-device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-
-curl = CURL(obs_shape, feature_dim, batch_size, pixel_encoder, pixel_encoder_target).to(device)
-
-
-curl.eval()
-
-if conf['curl']['load']:
-    weights = torch.load(path_weights / conf['experiment'] / conf['curl']['epoch'])['state_dict']
-    curl.load_state_dict(weights)
-
-def save_image(img, name):
-    fig, ax = plt.subplots()
-    plt.imsave(f'../images/inference_attention_2/{name}.png',img)
-    plt.close()
-
-def save_fig(img, name):
-    plt.imshow(img)
-    plt.axis('off')
-    plt.savefig(f'../images/inference_attention_2/{name}.svg')
-    plt.close()
-
-for i, (current_state, action, reward, next_state, done) in enumerate(data.batch_iter(batch_size=1, num_epochs=3, seq_len=10)):
-
-    # batch = current_state['pov']
-    #
-    # obs_anchor = batch[:,0,:,:,:]
-    # obs_pos = batch[:,-1,:,:,:]
-    #
-    # obs_anchor = obs_anchor[5]
-
-    # part 0 (store image trajectories every x samples)
-    # for j, b in enumerate(batch[0]):
-    #     if j%20==0:
-    #         save_image(b, j)
-    # ---------------
-
-    # save_image(obs_anchor, 'original_image')
-    # obs_anchor = torch.from_numpy(obs_anchor).unsqueeze(dim=0).float().to(device)
-    #
-    # obs_anchor = obs_anchor.permute(0,3,1,2)
-
-    # part 1 store convolutional outputs
-    # h, conv = curl.encoder.forward_conv(obs_anchor)
-    # for j,c in enumerate(conv[0]):
-    #     img = c.detach().cpu().numpy()
-    #     save_image(img, j)
-    # ---------
-
-    # # part 2 store embeddings reshaped
-    # z_a = curl.encode(obs_anchor)
-    # z = z_a[0][:49].detach().cpu().numpy()
-    # z = z.reshape(7,7)
-    # save_fig(z, 'z_reshaped')
-    # zw = z_a*curl.W
-    # zw = zw.detach().cpu().numpy()
-    # zw = np.diag(zw)[:49]
-    # zw = zw.reshape(7,7)
-    # save_fig(zw, 'zxW')
-    # -----------
-
-    # break
-print('\n')
diff --git a/src/curl_training.py b/src/curl_training.py
deleted file mode 100644
index 8255b6d..0000000
--- a/src/curl_training.py
+++ /dev/null
@@ -1,123 +0,0 @@
-import os
-import sys
-import cv2
-import gym
-import json
-import time
-import minerl
-import numpy as np
-import matplotlib.pyplot as plt
-
-from os.path import join
-from pathlib import Path
-from pprint import pprint
-
-from config import setSeed, getConfig
-from random_shift import random_shift
-
-import torch
-import torch.nn as nn
-import torch.optim as optim
-import torch.nn.functional as F
-
-from torch.utils.data import DataLoader
-from torch.utils.tensorboard import SummaryWriter
-
-from encoder import PixelEncoder
-from CURL import CURL
-
-from IPython import embed
-
-setSeed(0)
-assert len(sys.argv) == 2, "Indicate a configuration file like 'config_0.0'"
-conf = getConfig(sys.argv[1])
-
-MINERL_GYM_ENV = os.getenv('MINERL_GYM_ENV', 'MineRLNavigate-v0')
-# MINERL_GYM_ENV = os.getenv('MINERL_GYM_ENV', 'MineRLTreechop-v0')
-MINERL_DATA_ROOT = os.getenv('MINERL_DATA_ROOT', '/home/usuaris/imatge/juan.jose.nieto/mineRL/data/')
-data = minerl.data.make(MINERL_GYM_ENV, data_dir=MINERL_DATA_ROOT, num_workers=1)
-
-feature_dim = conf['curl']['embedding_dim']
-img_size = conf['curl']['img_size']
-obs_shape = (3, img_size, img_size)
-
-if os.getenv('USER') == 'juanjo':
-    path_weights = Path('../weights/')
-elif os.getenv('USER') == 'juan.jose.nieto':
-    path_weights = Path('/mnt/gpid07/users/juan.jose.nieto/weights/')
-else:
-    raise Exception("Sorry user not identified!")
-
-if not os.path.exists(path_weights / conf['experiment']):
-	os.mkdir(path_weights / conf['experiment'])
-
-pixel_encoder = PixelEncoder(obs_shape, feature_dim)
-pixel_encoder_target = PixelEncoder(obs_shape, feature_dim)
-
-device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-
-batch_size = conf['batch_size']
-tau = conf['curl']['encoder_tau']
-
-curl = CURL(obs_shape, feature_dim, batch_size, pixel_encoder, pixel_encoder_target).to(device)
-optimizer = optim.Adam(curl.encoder.parameters(), lr=conf['learning_rate'], amsgrad=False)
-optimizer_full = optim.Adam(curl.parameters(), lr=conf['learning_rate'], amsgrad=False)
-
-writer = SummaryWriter(log_dir=f"../tensorboard/{conf['experiment']}/")
-
-curl.train()
-
-def soft_update_params(net, target_net, tau):
-    for param, target_param in zip(net.parameters(), target_net.parameters()):
-        target_param.data.copy_(
-            tau * param.data + (1 - tau) * target_param.data
-        )
-
-def saveModel(path, exp, model, optim, iter):
-	file_name = str(iter) + '.pt'
-	path = path / exp / file_name
-	torch.save({
-        'state_dict': model.state_dict(),
-		'optimizer': optim},
-		path)
-
-def save_image(j, i):
-    img = np.concatenate((j[0], j[-1]), axis=1)
-    fig, ax = plt.subplots()
-    plt.imsave(f'./images/curl_sampled/{i}.png',img)
-    plt.close()
-
-for i, (current_state, action, reward, next_state, done) in enumerate(data.batch_iter(batch_size=batch_size, num_epochs=conf['epochs'], seq_len=conf['seq_len'])):
-    batch = current_state['pov']
-    obs_anchor = batch[:,0,:,:,:]
-    obs_pos = batch[:,-1,:,:,:]
-
-    obs_anchor = random_shift(obs_anchor, pad=4)
-    obs_pos = random_shift(obs_pos, pad=4)
-
-    obs_anchor = torch.from_numpy(obs_anchor).float().squeeze().to(device)
-    obs_pos = torch.from_numpy(obs_pos).float().squeeze().to(device)
-
-    obs_anchor = obs_anchor.permute(0,3,1,2)
-    obs_pos = obs_pos.permute(0,3,1,2)
-
-    z_a = curl.encode(obs_anchor)
-    z_pos = curl.encode(obs_pos, ema=True)
-
-
-    logits = curl.compute_logits(z_a, z_pos)
-    labels = torch.arange(logits.shape[0]).long().to(device)
-    loss = torch.nn.CrossEntropyLoss()(logits, labels)
-    optimizer.zero_grad()
-    optimizer_full.zero_grad()
-    loss.backward()
-
-    optimizer.step()
-    optimizer_full.step()
-    writer.add_scalar('CURL/Loss', loss.item(), i)
-
-    if i%2==0:
-        soft_update_params(curl.encoder, curl.encoder_target, tau)
-
-    if i%5000==0:
-        saveModel(path_weights, conf['experiment'], curl, optimizer, i)
diff --git a/src/encoder.py b/src/encoder.py
deleted file mode 100644
index b1eb4b5..0000000
--- a/src/encoder.py
+++ /dev/null
@@ -1,126 +0,0 @@
-import torch
-import torch.nn as nn
-from IPython import embed
-
-def tie_weights(src, trg):
-    assert type(src) == type(trg)
-    trg.weight = src.weight
-    trg.bias = src.bias
-
-
-# for 84 x 84 inputs
-OUT_DIM = {2: 39, 4: 35, 6: 31}
-# for 64 x 64 inputs
-OUT_DIM_64 = {2: 29, 4: 25, 6: 21}
-
-
-class PixelEncoder(nn.Module):
-    """Convolutional encoder of pixels observations."""
-    def __init__(self, obs_shape, feature_dim, num_layers=2, num_filters=32,output_logits=False):
-        super().__init__()
-
-        assert len(obs_shape) == 3
-        self.obs_shape = obs_shape
-        self.feature_dim = feature_dim
-        self.num_layers = num_layers
-
-        self.convs = nn.ModuleList(
-            [nn.Conv2d(obs_shape[0], num_filters, 3, stride=2)]
-        )
-        for i in range(num_layers - 1):
-            self.convs.append(nn.Conv2d(num_filters, num_filters, 3, stride=1))
-
-        out_dim = OUT_DIM_64[num_layers] if obs_shape[-1] == 64 else OUT_DIM[num_layers]
-        self.fc = nn.Linear(num_filters * out_dim * out_dim, self.feature_dim)
-        self.ln = nn.LayerNorm(self.feature_dim)
-
-        self.outputs = dict()
-        self.output_logits = output_logits
-
-    def reparameterize(self, mu, logstd):
-        std = torch.exp(logstd)
-        eps = torch.randn_like(std)
-        return mu + eps * std
-
-    def forward_conv(self, obs):
-        obs = obs / 255.
-        self.outputs['obs'] = obs
-
-        conv = torch.relu(self.convs[0](obs))
-        self.outputs['conv1'] = conv
-
-        for i in range(1, self.num_layers):
-            conv = torch.relu(self.convs[i](conv))
-            self.outputs['conv%s' % (i + 1)] = conv
-
-        h = conv.contiguous().view(conv.size(0), -1)
-        return h
-
-    def forward(self, obs, detach=False):
-        h = self.forward_conv(obs)
-
-        if detach:
-            h = h.detach()
-
-        h_fc = self.fc(h)
-        self.outputs['fc'] = h_fc
-
-        h_norm = self.ln(h_fc)
-        self.outputs['ln'] = h_norm
-
-        if self.output_logits:
-            out = h_norm
-        else:
-            out = torch.tanh(h_norm)
-            self.outputs['tanh'] = out
-
-        return out
-
-    def copy_conv_weights_from(self, source):
-        """Tie convolutional layers"""
-        # only tie conv layers
-        for i in range(self.num_layers):
-            tie_weights(src=source.convs[i], trg=self.convs[i])
-
-    def log(self, L, step, log_freq):
-        if step % log_freq != 0:
-            return
-
-        for k, v in self.outputs.items():
-            L.log_histogram('train_encoder/%s_hist' % k, v, step)
-            if len(v.shape) > 2:
-                L.log_image('train_encoder/%s_img' % k, v[0], step)
-
-        for i in range(self.num_layers):
-            L.log_param('train_encoder/conv%s' % (i + 1), self.convs[i], step)
-        L.log_param('train_encoder/fc', self.fc, step)
-        L.log_param('train_encoder/ln', self.ln, step)
-
-
-class IdentityEncoder(nn.Module):
-    def __init__(self, obs_shape, feature_dim, num_layers, num_filters,*args):
-        super().__init__()
-
-        assert len(obs_shape) == 1
-        self.feature_dim = obs_shape[0]
-
-    def forward(self, obs, detach=False):
-        return obs
-
-    def copy_conv_weights_from(self, source):
-        pass
-
-    def log(self, L, step, log_freq):
-        pass
-
-
-_AVAILABLE_ENCODERS = {'pixel': PixelEncoder, 'identity': IdentityEncoder}
-
-
-def make_encoder(
-    encoder_type, obs_shape, feature_dim, num_layers, num_filters, output_logits=False
-):
-    assert encoder_type in _AVAILABLE_ENCODERS
-    return _AVAILABLE_ENCODERS[encoder_type](
-        obs_shape, feature_dim, num_layers, num_filters, output_logits
-    )

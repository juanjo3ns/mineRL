{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "\n",
    "from models.VQVAE import VectorQuantizerEMA, Encoder, Decoder\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "from customLoader import CustomMinecraftData\n",
    "from torchvision.transforms import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(pl.LightningModule):\n",
    "    def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens,\n",
    "                 num_embeddings, embedding_dim, commitment_cost, decay=0,\n",
    "                 batch_size=256, lr=0.001, split=0.95, img_size=64):\n",
    "        super(VQVAE, self).__init__()\n",
    "\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.split = split\n",
    "\n",
    "        self._encoder = Encoder(3, num_hiddens,\n",
    "                                num_residual_layers,\n",
    "                                num_residual_hiddens)\n",
    "        # self._pre_vq_conv = nn.Conv2d(in_channels=num_hiddens,\n",
    "        #                               out_channels=embedding_dim,\n",
    "        #                               kernel_size=1,\n",
    "        #                               stride=1)\n",
    "        if decay > 0.0:\n",
    "            self._vq_vae = VectorQuantizerEMA(num_embeddings, embedding_dim,\n",
    "                                              commitment_cost, decay)\n",
    "        else:\n",
    "            self._vq_vae = VectorQuantizer(num_embeddings, embedding_dim,\n",
    "                                           commitment_cost)\n",
    "        self._decoder = Decoder(num_hiddens,\n",
    "                                num_hiddens,\n",
    "                                num_residual_layers,\n",
    "                                num_residual_hiddens)\n",
    "\n",
    "        self.example_input_array = torch.rand(batch_size, 3, img_size, img_size)\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.5,0.5,0.5), (1.0,1.0,1.0))\n",
    "                                ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self._encoder(x)\n",
    "        # z = self._pre_vq_conv(z)\n",
    "        loss, quantized, perplexity, _ = self._vq_vae(z)\n",
    "        x_recon = self._decoder(quantized)\n",
    "\n",
    "        return loss, x_recon, perplexity\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        vq_loss, data_recon, perplexity = self(batch)\n",
    "        recon_error = F.mse_loss(data_recon, batch)\n",
    "        loss = recon_error + vq_loss\n",
    "\n",
    "        self.log('loss/train', loss, on_step=False, on_epoch=True)\n",
    "        self.log('perplexity/train', perplexity, on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        vq_loss, data_recon, perplexity = self(batch)\n",
    "        recon_error = F.mse_loss(data_recon, batch)\n",
    "        loss = recon_error + vq_loss\n",
    "\n",
    "        self.log('loss/val', loss, on_step=False, on_epoch=True)\n",
    "        self.log('perplexity/val', perplexity, on_step=False, on_epoch=True)\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            grid = make_grid(data_recon[:64].cpu().data)\n",
    "            grid = grid.permute(1,2,0)\n",
    "            self.logger.experiment.log({\"Images\": [wandb.Image(grid.numpy())]})\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(params=self.parameters(), lr=self.lr, weight_decay=1e-5)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = CustomMinecraftData('CustomTrajectories1', 'train', self.split, transform=self.transform)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = CustomMinecraftData('CustomTrajectories1', 'val', self.split, transform=self.transform)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=2)\n",
    "        return val_dataloader\n",
    "\n",
    "\n",
    "    def get_centroids(self, idx):\n",
    "        z_idx = torch.tensor(idx).cuda()\n",
    "        embeddings = torch.index_select(self._vq_vae._embedding.weight.detach(), dim=0, index=z_idx)\n",
    "        embeddings = embeddings.view((1,2,2,64))\n",
    "        embeddings = embeddings.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        return self._decoder(embeddings)\n",
    "\n",
    "    def save_encoding_indices(self, x):\n",
    "        z = self._encoder(x)\n",
    "        z = self._pre_vq_conv(z)\n",
    "        _, _, _, encoding_indices = self._vq_vae(z)\n",
    "        return encoding_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "  'split': 0.95,\n",
    "  'lr': 0.001,\n",
    "  'batch_size': 256,\n",
    "  'num_hiddens': 64,\n",
    "  'num_residual_hiddens': 32,\n",
    "  'num_residual_layers': 2,\n",
    "  'embedding_dim': 256,\n",
    "  'num_embeddings': 9,\n",
    "  'commitment_cost': 0.25,\n",
    "  'decay': 0.99\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VQVAE(\n",
       "  (_encoder): Encoder(\n",
       "    (_conv_1): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (_conv_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (_conv_3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (_conv_4): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (_conv_5): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (_residual_stack): ResidualStack(\n",
       "      (_layers): ModuleList(\n",
       "        (0): Residual(\n",
       "          (_block): Sequential(\n",
       "            (0): ReLU(inplace=True)\n",
       "            (1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (_block): Sequential(\n",
       "            (0): ReLU(inplace=True)\n",
       "            (1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (_vq_vae): VectorQuantizerEMA(\n",
       "    (_embedding): Embedding(9, 256)\n",
       "  )\n",
       "  (_decoder): Decoder(\n",
       "    (_residual_stack): ResidualStack(\n",
       "      (_layers): ModuleList(\n",
       "        (0): Residual(\n",
       "          (_block): Sequential(\n",
       "            (0): ReLU(inplace=True)\n",
       "            (1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (_block): Sequential(\n",
       "            (0): ReLU(inplace=True)\n",
       "            (1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (_conv_trans_1): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (_conv_trans_2): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (_conv_trans_3): ConvTranspose2d(16, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (_conv_trans_4): ConvTranspose2d(16, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (_conv_trans_5): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqvae = VQVAE(**conf).cuda()\n",
    "vqvae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4289,  1.3668, -0.2483,  ...,  1.5809,  0.1880,  0.0234],\n",
       "        [-0.3491,  0.0706, -0.5293,  ..., -1.1863,  0.3065, -0.8955],\n",
       "        [ 1.0672,  0.6393,  0.4253,  ..., -1.2144, -0.1740,  0.3782],\n",
       "        ...,\n",
       "        [ 1.1277, -0.2112, -1.0604,  ...,  0.3409, -0.3557, -0.0213],\n",
       "        [ 0.0105,  0.6967,  0.6859,  ..., -0.4385,  1.3386,  1.5643],\n",
       "        [ 0.7134, -1.5994,  1.9780,  ...,  0.1101,  1.4509,  1.0463]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqvae._vq_vae._embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../results/vqvae_0.2/mineRL/y77fc26u/checkpoints/epoch=808-step=61483.ckpt'\n",
    "\n",
    "path = '../results/vqvae_0.1/mineRL/2wgoga4p/checkpoints/epoch=833-step=62549.ckpt'\n",
    "path = '../results/vqvae_0.3/mineRL/1c4o6jgy/checkpoints/epoch=499-step=37999.ckpt'\n",
    "path = '../results/vqvae_2.0/mineRL/kbsmulhw/checkpoints/epoch=49-step=4499.ckpt'\n",
    "checkpoint = torch.load(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqvae.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 9.3229e-04,  1.8156e-05,  1.7662e-07,  ...,  1.1470e-08,\n",
       "          9.5532e-02,  1.2836e-15],\n",
       "        [ 1.2524e-02,  1.0146e-05,  2.2878e-07,  ...,  4.2857e-09,\n",
       "          1.5041e-03,  1.2887e-06],\n",
       "        [ 2.7098e-04,  1.0980e-06,  1.1503e-12,  ...,  1.9750e-09,\n",
       "          3.2994e-01,  5.1746e-18],\n",
       "        ...,\n",
       "        [ 1.0920e-01,  1.2528e-05,  1.4541e-09,  ..., -1.1841e-21,\n",
       "          2.0193e-03,  6.6190e-08],\n",
       "        [ 1.5943e-02,  2.6290e-05,  1.0618e-09,  ...,  2.8486e-10,\n",
       "          1.5882e-04,  2.3249e-08],\n",
       "        [ 4.4367e-03,  1.3830e-05,  3.7567e-04,  ...,  1.1604e-09,\n",
       "          3.4997e-03,  2.0850e-07]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqvae._vq_vae._embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    out = vqvae.get_centroids(i)\n",
    "    img = out.squeeze().permute(1,2,0).detach().cpu().numpy()\n",
    "    img = img + 0.5\n",
    "    img[img>1] = 1\n",
    "    #plt.imshow(img)\n",
    "    #plt.show()\n",
    "    plt.imsave(f\"../goal_states/sweep_vqvae/centroid_{i}.png\", img)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

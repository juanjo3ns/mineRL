{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "\n",
    "from models.VQVAE import VectorQuantizerEMA, Encoder, Decoder\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "from customLoader import CustomMinecraftData\n",
    "from torchvision.transforms import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(pl.LightningModule):\n",
    "    def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens,\n",
    "                 num_embeddings, embedding_dim, commitment_cost, decay=0,\n",
    "                 batch_size=256, lr=0.001, split=0.95, img_size=64):\n",
    "        super(VQVAE, self).__init__()\n",
    "\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.split = split\n",
    "\n",
    "        self._encoder = Encoder(3, num_hiddens,\n",
    "                                num_residual_layers,\n",
    "                                num_residual_hiddens)\n",
    "        # self._pre_vq_conv = nn.Conv2d(in_channels=num_hiddens,\n",
    "        #                               out_channels=embedding_dim,\n",
    "        #                               kernel_size=1,\n",
    "        #                               stride=1)\n",
    "        if decay > 0.0:\n",
    "            self._vq_vae = VectorQuantizerEMA(num_embeddings, embedding_dim,\n",
    "                                              commitment_cost, decay)\n",
    "        else:\n",
    "            self._vq_vae = VectorQuantizer(num_embeddings, embedding_dim,\n",
    "                                           commitment_cost)\n",
    "        self._decoder = Decoder(num_hiddens,\n",
    "                                num_hiddens,\n",
    "                                num_residual_layers,\n",
    "                                num_residual_hiddens)\n",
    "\n",
    "        self.example_input_array = torch.rand(batch_size, 3, img_size, img_size)\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.5,0.5,0.5), (1.0,1.0,1.0))\n",
    "                                ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self._encoder(x)\n",
    "        # z = self._pre_vq_conv(z)\n",
    "        loss, quantized, perplexity, _ = self._vq_vae(z)\n",
    "        x_recon = self._decoder(quantized)\n",
    "\n",
    "        return loss, x_recon, perplexity\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        vq_loss, data_recon, perplexity = self(batch)\n",
    "        recon_error = F.mse_loss(data_recon, batch)\n",
    "        loss = recon_error + vq_loss\n",
    "\n",
    "        self.log('loss/train', loss, on_step=False, on_epoch=True)\n",
    "        self.log('perplexity/train', perplexity, on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        vq_loss, data_recon, perplexity = self(batch)\n",
    "        recon_error = F.mse_loss(data_recon, batch)\n",
    "        loss = recon_error + vq_loss\n",
    "\n",
    "        self.log('loss/val', loss, on_step=False, on_epoch=True)\n",
    "        self.log('perplexity/val', perplexity, on_step=False, on_epoch=True)\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            grid = make_grid(data_recon[:64].cpu().data)\n",
    "            grid = grid.permute(1,2,0)\n",
    "            self.logger.experiment.log({\"Images\": [wandb.Image(grid.numpy())]})\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(params=self.parameters(), lr=self.lr, weight_decay=1e-5)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = CustomMinecraftData('CustomTrajectories1', 'train', self.split, transform=self.transform)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = CustomMinecraftData('CustomTrajectories1', 'val', self.split, transform=self.transform)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=2)\n",
    "        return val_dataloader\n",
    "\n",
    "\n",
    "    def get_centroids(self, idx):\n",
    "        z_idx = torch.tensor(idx).cuda()\n",
    "        embeddings = torch.index_select(self._vq_vae._embedding.weight.detach(), dim=0, index=z_idx)\n",
    "        embeddings = embeddings.view((1,2,2,64))\n",
    "        embeddings = embeddings.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        return self._decoder(embeddings)\n",
    "\n",
    "    def save_encoding_indices(self, x):\n",
    "        z = self._encoder(x)\n",
    "        z = self._pre_vq_conv(z)\n",
    "        _, _, _, encoding_indices = self._vq_vae(z)\n",
    "        return encoding_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "  'split': 0.95,\n",
    "  'lr': 0.001,\n",
    "  'batch_size': 256,\n",
    "  'num_hiddens': 64,\n",
    "  'num_residual_hiddens': 32,\n",
    "  'num_residual_layers': 2,\n",
    "  'embedding_dim': 256,\n",
    "  'num_embeddings': 10,\n",
    "  'commitment_cost': 0.25,\n",
    "  'decay': 0.99\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VQVAE(\n",
       "  (_encoder): Encoder(\n",
       "    (_conv_1): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (_conv_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (_conv_3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (_conv_4): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (_conv_5): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (_residual_stack): ResidualStack(\n",
       "      (_layers): ModuleList(\n",
       "        (0): Residual(\n",
       "          (_block): Sequential(\n",
       "            (0): ReLU(inplace=True)\n",
       "            (1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (_block): Sequential(\n",
       "            (0): ReLU(inplace=True)\n",
       "            (1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (_vq_vae): VectorQuantizerEMA(\n",
       "    (_embedding): Embedding(10, 256)\n",
       "  )\n",
       "  (_decoder): Decoder(\n",
       "    (_residual_stack): ResidualStack(\n",
       "      (_layers): ModuleList(\n",
       "        (0): Residual(\n",
       "          (_block): Sequential(\n",
       "            (0): ReLU(inplace=True)\n",
       "            (1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (_block): Sequential(\n",
       "            (0): ReLU(inplace=True)\n",
       "            (1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (_conv_trans_1): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (_conv_trans_2): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (_conv_trans_3): ConvTranspose2d(16, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (_conv_trans_4): ConvTranspose2d(16, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (_conv_trans_5): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqvae = VQVAE(**conf).cuda()\n",
    "vqvae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4548, -1.6205,  0.9078,  ...,  2.1332, -2.9290,  0.2161],\n",
       "        [-0.1996,  0.4195, -0.6094,  ...,  0.7398, -0.9086,  0.3735],\n",
       "        [-2.0932,  2.0800, -1.5830,  ...,  0.3970, -2.2560,  0.7920],\n",
       "        ...,\n",
       "        [ 1.7407,  0.2839,  0.6586,  ..., -2.5021, -0.2781,  0.6295],\n",
       "        [ 0.5438,  1.0325,  0.5431,  ..., -1.2149,  0.4331,  0.1137],\n",
       "        [-0.1712, -0.0969, -0.1637,  ..., -0.6019, -0.5315,  0.1484]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqvae._vq_vae._embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../results/vqvae_0.2/mineRL/y77fc26u/checkpoints/epoch=808-step=61483.ckpt'\n",
    "\n",
    "path = '../results/vqvae_0.1/mineRL/2wgoga4p/checkpoints/epoch=833-step=62549.ckpt'\n",
    "path = '../results/vqvae_0.3/mineRL/1c4o6jgy/checkpoints/epoch=499-step=37999.ckpt'\n",
    "checkpoint = torch.load(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqvae.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.0807, 0.1703, 0.1083,  ..., 0.0195, 0.4289, 0.1417],\n",
       "        [0.1158, 0.2472, 0.3529,  ..., 1.0685, 0.2352, 0.1642],\n",
       "        [0.0281, 1.5890, 1.1100,  ..., 2.0561, 0.2099, 0.2285],\n",
       "        ...,\n",
       "        [0.0887, 0.4713, 0.4223,  ..., 0.3388, 0.3095, 0.1533],\n",
       "        [0.0366, 0.5491, 0.3956,  ..., 0.7268, 0.3135, 0.1216],\n",
       "        [0.0397, 1.0619, 0.7591,  ..., 1.2831, 0.2222, 0.1758]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqvae._vq_vae._embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    out = vqvae.get_centroids(i)\n",
    "    img = out.squeeze().permute(1,2,0).detach().cpu().numpy()\n",
    "    img = img + 0.5\n",
    "    img[img>1] = 1\n",
    "    #plt.imshow(img)\n",
    "    #plt.show()\n",
    "    plt.imsave(f\"../goal_states/flat_biome_vqvae/centroid_{i}.png\", img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
